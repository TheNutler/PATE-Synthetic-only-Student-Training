# ğŸ“˜ Thesis Notes â€” Structured Summary

---

## ğŸ¯ Research Questions

**RQ1 â€” Predictive Performance**  
To what extent does training a student model *exclusively on synthetic data generated by teacher models* preserve predictive performance compared to the standard PATE framework?

**RQ2 â€” Privacy Budget**  
How much privacy budget (Ïµ, Î´ under differential privacy) is consumed by the proposed approach, and how does it compare to standard PATE?

**RQ3 â€” Data Efficiency**  
How much synthetic data is required to reach a defined performance threshold on a downstream classification task?

**RQ4 â€” Distributional Fidelity**  
To what degree does the combined synthetic dataset replicate the distributional properties (feature coverage, label balance, variance structure) of the original sensitive data?

---

## ğŸ” How Privacy Was Measured (Qualitative & Empirical)

- **Nearest-neighbor memorization checks**  
  Ensures synthetic samples are not too close to any real private examples.

- **Cross-set distance analysis**  
  Measures resemblance between private and synthetic datasets.

- **Diversity ratio**  
  Verifies that the generator does not collapse toward a few modes or private samples.

- **Rejection of overly similar synthetic samples**  
  A safety filter to prevent accidental leakage.

- **EMNIST-only decoder pretraining**  
  The decoder learns from a *public dataset only*, preventing any leakage of private MNIST teacher shards.

---

## ğŸ“Š How to Compare With Standard PATE

- **Standard PATE**
  - Uses *teacher voting* on real data  
  - Injects *DP noise* into vote counts  
  - Privacy is quantified with a numerical budget **Ïµ, Î´**
  - Requires *public auxiliary data* for student training

- **Your Synthetic-Only PATE Variant**
  - Teachers **do not vote**  
  - Teachers **only label synthetic data**, never real data  
  - No DP noise required for voting â†’ *no accumulation of Ïµ*  
  - Privacy is **structural**:
    - No student access to real data  
    - No labeling of real data  
    - Synthetic samples checked for novelty and diversity  

**Conclusion:**  
Your method provides *empirical and structural privacy* rather than formal (Ïµ-based) guarantees, with strong practical evidence of **no memorization** and **high sample novelty**.

---

## âš ï¸ Problems With Per-Teacher VAEs

Why per-teacher VAE training fails:

- Only **~240 samples** per teacher â†’ far too few  
- Strong **underfitting**  
- Blurry, low-quality reconstructions  
- Almost no diversity  
- **Mode collapse** across labels  
- High **memorization risk**  
- Severe class imbalance in tiny shards  
- Training **250 VAEs** is computationally prohibitive

---

## âœ… Why Pretrained Decoder Fixes Everything

Using one pretrained decoder trained on **112k EMNIST samples** gives major benefits:

- High reconstruction quality  
- Learns **general handwriting**, not specific MNIST digits  
- No memorization of private data  
- Teachers add label semantics â†’ synthetic labels remain correct  
- Only **one shared model** â†’ efficient  
- Produces a high-diversity, high-quality synthetic dataset

---

## ğŸ§  Why the Pretrained Decoder is Privacy-Safe

Your pretrained decoder:

- âœ” Is trained **only on EMNIST**, not private MNIST shards  
- âœ” Learns general handwriting priors, not user-specific patterns  
- âœ” Is reused across all teachers â€” no teacher-specific memorization  
- âœ” Is never influenced by any private subset  
- âœ” Provides a robust base generator for all synthetic samples  
- âœ” Avoids data leakage by design

---
